# Mechanistic Interpretability Beyond Language Models

## Abstract

In the recent times, a plethora of *mechanistic* interpretability methods have been developed, used towards understanding and modifying different aspects of language models. However, progress towards understanding *multimodal* and *vision* models have been slow when compared to the interest in mechanistic interpretability for language models.
In the past one year, Reliable AI Lab has been developing and adapting interpretability tools to understand the inner workings of multimodal and vision models, with a downstream emphasis on applications such as
+ removing copyrighted content in T2I models
+ improving compositionality
+ mitigating spurious correlations in vision models
+ updating rare knowledge in MLLMs

This blog provides a comprehensive overview of our year-long efforts in mechanistic interpretability beyond language models and discusses the open research problems in this area.


## <span style="font-variant: small-caps;">Causal Tracing</span> For Text-to-Image Generative Models
In language models, prior work has shown that factual knowledge can be effectively localized to a small set of layers. These layers are localized via causal interventions which measure the change in model outputs by perturbing an internal layer. In this project, we adapt causal interventions to text-to-image diffusion models to understand if knowledge about various visual attributes can be localized to a small number of layers. The purpose of mechanistically understanding generative models is two-fold: 

1. A mechanistic understanding of generative models about how they represent knowledge can uncover novel scientific understanding about their inner workings.
2. Understanding the inner workings of diffusion models can help us design effective unlearning strategies towards preventing the model towards generating copyrighted content (e.g., certain artistic styles).

We apply causal interventions in the UNet (noise-prediction module) of the diffusion model as well as the text-encoder. In the UNet, we find that knowledge about different visual attributes is distributed. However, we find that this distribution across layers is highly dependent on the visual attribute. For example, knowledge about style is distributed differently than knowledge about action in the images.

In the text-encoder, we surprisingly find only one causal state which can control the output of generative models. In particular this is the first self-attention layer. As shown below, we find that this layer can control various visual attributes such as copyrighted styles, objects and general factual knowledge. 

Using these interpretability insights, we then develop DiffQuickFix ‚Äî a fast and effective model editing algorithm to prevent diffusion models from generating copyrighted content. The idea of DiffQuickFix is simple: update the projection layer in the causal first self-attention layer in the CLIP text-encoder such to create new associations between certain keys (e.g., Van Gogh style) and values (e.g., painting). In essence, post-editing the text-encoder, the model will always map a ‚ÄòVan Gogh‚Äô style to a generic painting which will ensure that the diffusion model is not able to generate the copyrighted content.  Finally, we note that DiffQuickFix is extremely fast and it can edit concepts in under 1 seconds, almost a 1000x speedup over various fine-tuning based unlearning methods. 

Below we show examples of our model editing algorithm: 


üìö <small> **Paper**: ["Localizing and Editing Knowledge in Text-to-Image Generative Models"](https://openreview.net/pdf?id=Qmw9ne6SOQ) </small><br>
üåê <small> **Webpage**: [Explore!](https://samyadeepbasu.github.io/causal-knowledge-localization.github.io/)</small>

## <span style="font-variant: small-caps;">Localizing Knowledge</span>  in Cross-Attention Layers

Given the limitations of causal tracing for localizing knowledge within text encoders of recent models, we sought to develop a more generalizable approach for knowledge localization in text-to-image models‚Äîone that could be effectively scaled and applied to modern architectures. In this work, we investiage whether knowledge representing *artistic styles*, *objects*, or *facts* can be localized in cross-attention layers of text-to-image models. Interestingly, we observed that among significant number of cross attention layers, only a select few play a key role in controlling the generation of specific concepts. In fact, the model heavily relies on information provided by those layers to generate those concepts. *e.g.*, we observed that in Stable Diffusion XL, layers 45-48 among 64 layers are responsible for Van Gogh style. Modifying the input only in these specific layers leads the UNet to produce an image that noticeably loses its Van Gogh characteristics. Localization within few layers enables an efficient and surgical model editiing method that aims to minimally modify cross-attention layers in those specified layers. This happens by carefully editing key value matrices within those layers, mapping original text prompts to representations that exclude the targeted concept. Importantly, the edited models retain their general utility and continue to generate high-quality images when prompted with general inputs. This approach worked for SD-v1.4, SD-v2, SD-XL, DeepFloyd and OpenJourney!

üìö <small> **Paper**: ["On Mechanistic Knowledge Localization in Text-to-Image Generative Models"](https://proceedings.mlr.press/v235/basu24b.html) </small><br>
üåê <small> **Webpage**: [Explore!](https://t2i-knowledge-localization.github.io)</small>

#### Future Directions

Say something here!

## Mechanistically Understand Model Components Across Different ViTs

In text-to-image models, we were able to analyze and control the generation of concepts via causal tracing by perturbing carefully chosen text tokens in the input which correspond to the concept of interest. However, in vision models with image inputs, it is more challenging to carefully control and perturb the chosen concepts, which means that causal tracing is less effective in this setting. Thus, rather than understanding the model from the input side, we aim to understand the model from the representation side. Specifically, we use the following approach:

1. We decompose the final representation as a sum of contributions from different model components. Furthermore, each of these component contributions can be decomposed over the image patch tokens.
2. We then map each of these contributions to CLIP space where they can be interpreted using CLIP's shared image-text representation space
3. Using the CLIP text encoder, we identify which components are responsible for encoding which concepts, and use them to retrieve or segment images, or ablate them to mitigate spurious correlations

üìö <small> **Paper**: ["Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP"](https://openreview.net/forum?id=Vhh7ONtfvV) </small><br>
üåê <small> **Webpage**: [Explore!](https://sriram.live/vit-decompose/)</small>

#### Future Directions

Although we were able to localize certain concepts via this approach, there are still many open questions. For one, we found that the direct controbutions of model components are not limited to one role - they are responsible for jointly encoding many concepts along with other concepts. This makes it difficult to ablate them without affecting other concepts. One way to address this is to probably select a subspace within each component that is responsible for encoding a single concept. Another direction is to understand how these components interact with each other to encode concepts, as it is likely that concept components are built out of simpler ones.


## Compositionality in Diffusion Models

## Understanding Internal Components in Multimodal Language Models

