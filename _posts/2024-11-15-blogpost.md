# Mechanistic Interpretability Beyond Language Models

## Abstract

In the recent times, a plethora of *mechanistic* interpretability methods have been developed, used towards understanding and modifying different aspects of language models. However, progress towards understanding *multimodal* and *vision* models have been slow when compared to the interest in mechanistic interpretability for language models.
In the past one year, Reliable AI Lab has been developing and adapting interpretability tools to understand the inner workings of multimodal and vision models, with a downstream emphasis on applications such as
+ removing copyrighted content in T2I models
+ improving compositionality
+ mitigating spurious correlations in vision models
+ updating rare knowledge in MLLMs

This blog provides a comprehensive overview of our year-long efforts in mechanistic interpretability beyond language models and discusses the open research problems in this area.


## <span style="font-variant: small-caps;">Causal Tracing</span> For Text-to-Image Generative Models
In language models, prior work has shown that factual knowledge can be effectively localized to a small set of layers. These layers are localized via causal interventions which measure the change in model outputs by perturbing an internal layer. In this project, we adapt causal interventions to text-to-image diffusion models to understand if knowledge about various visual attributes can be localized to a small number of layers. The purpose of mechanistically understanding generative models is two-fold: 

1. A mechanistic understanding of generative models about how they represent knowledge can uncover novel scientific understanding about their inner workings.
2. Understanding the inner workings of diffusion models can help us design effective unlearning strategies towards preventing the model towards generating copyrighted content (e.g., certain artistic styles).

We apply causal interventions in the UNet (noise-prediction module) of the diffusion model as well as the text-encoder. In the UNet, we find that knowledge about different visual attributes is distributed. However, we find that this distribution across layers is highly dependent on the visual attribute. For example, knowledge about style is distributed differently than knowledge about action in the images.

![Alt text](/_posts/images/Unet.png "Knowledge about visual attributes is distributed in the UNet")

In the text-encoder, we surprisingly find only one causal state which can control the output of generative models. In particular this is the first self-attention layer. As shown below, we find that this layer can control various visual attributes such as copyrighted styles, objects and general factual knowledge. 

Using these interpretability insights, we then develop DiffQuickFix ‚Äî a fast and effective model editing algorithm to prevent diffusion models from generating copyrighted content. The idea of DiffQuickFix is simple: update the projection layer in the causal first self-attention layer in the CLIP text-encoder such to create new associations between certain keys (e.g., Van Gogh style) and values (e.g., painting). In essence, post-editing the text-encoder, the model will always map a ‚ÄòVan Gogh‚Äô style to a generic painting which will ensure that the diffusion model is not able to generate the copyrighted content.  Finally, we note that DiffQuickFix is extremely fast and it can edit concepts in under 1 seconds, almost a 1000x speedup over various fine-tuning based unlearning methods. 

Below we show examples of our model editing algorithm: 


üìö <small> **Paper**: ["Localizing and Editing Knowledge in Text-to-Image Generative Models"](https://openreview.net/pdf?id=Qmw9ne6SOQ) </small><br>
üåê <small> **Webpage**: [Explore!](https://samyadeepbasu.github.io/causal-knowledge-localization.github.io/)</small>

## <span style="font-variant: small-caps;">Localizing Knowledge</span>  in Cross-Attention Layers

Given the limitations of causal tracing for localizing knowledge within text encoders of recent models, we sought to develop a more generalizable approach for knowledge localization in text-to-image models‚Äîone that could be effectively scaled and applied to modern architectures. In this work, we investiage whether knowledge representing *artistic styles*, *objects*, or *facts* can be localized in cross-attention layers of text-to-image models. Interestingly, we observed that among significant number of cross attention layers, only a select few play a key role in controlling the generation of specific concepts. In fact, the model heavily relies on information provided by those layers to generate those concepts. *e.g.*, we observed that in Stable Diffusion XL, layers 45-48 among 64 layers are responsible for Van Gogh style. Modifying the input only in these specific layers leads the UNet to produce an image that noticeably loses its Van Gogh characteristics. Localization within few layers enables an efficient and surgical model editiing method that aims to minimally modify cross-attention layers in those specified layers. This happens by carefully editing key value matrices within those layers, mapping original text prompts to representations that exclude the targeted concept. Importantly, the edited models retain their general utility and continue to generate high-quality images when prompted with general inputs. This approach worked for SD-v1.4, SD-v2, SD-XL, DeepFloyd and OpenJourney!

üìö <small> **Paper**: ["On Mechanistic Knowledge Localization in Text-to-Image Generative Models"](https://proceedings.mlr.press/v235/basu24b.html) </small><br>
üåê <small> **Webpage**: [Explore!](https://t2i-knowledge-localization.github.io)</small>

#### Future Directions

Say something here!

## Mechanistically Understand Model Components Across Different ViTs

In text-to-image models, we were able to analyze and control the generation of concepts via causal tracing by perturbing carefully chosen text tokens in the input which correspond to the concept of interest. However, in vision models with image inputs, it is more challenging to carefully control and perturb the chosen concepts, which means that causal tracing is less effective in this setting. Thus, rather than understanding the model from the input side, we aim to understand the model from the representation side. Specifically, we use the following approach:

1. We decompose the final representation as a sum of contributions from different model components. Furthermore, each of these component contributions can be decomposed over the image patch tokens.
2. We then map each of these contributions to CLIP space where they can be interpreted using CLIP's shared image-text representation space
3. Using the CLIP text encoder, we identify which components are responsible for encoding which concepts, and use them to retrieve or segment images, or ablate them to mitigate spurious correlations

üìö <small> **Paper**: ["Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP"](https://openreview.net/forum?id=Vhh7ONtfvV) </small><br>
üåê <small> **Webpage**: [Explore!](https://sriram.live/vit-decompose/)</small>

#### Future Directions

Although we were able to localize certain concepts via this approach, there are still many open questions. For one, we found that the direct controbutions of model components are not limited to one role - they are responsible for jointly encoding many concepts along with other concepts. This makes it difficult to ablate them without affecting other concepts. One way to address this is to probably select a subspace within each component that is responsible for encoding a single concept. Another direction is to understand how these components interact with each other to encode concepts, as it is likely that concept components are built out of simpler ones.


## Compositionality in Diffusion Models

## Understanding Internal Components in Multimodal Language Models

So far, we have adapted mechanistic interpretability methods for understanding text-to-image models and vision transformers. With the interest of the community of Multimodal Language Models for various tasks such as VQA, recaptioning datasets for training foundational models better ‚Äî we design and adapt methods to understand the inner workings of Multimodal Language Models such as LLava and Phi-vision.  In this project, we investigate how Multimodal Language models process information for a representative VQA task. To this end, we first curate a dataset which is annotated with ‚Äúconstraints‚Äù. For example, given an image of a Space Needle and a question, ‚ÄúWhere is this building located in‚Äù? ‚Äî the tokens corresponding to ‚Äúthis building‚Äù are constraints.  We curate a dataset called VQA-Constraints consisting of 9.7k questions (with their respective image) and annotate them with constraints.  With this richly annotated dataset, we answer the following questions:

1. How does the language part of the MLLM retrieve information corresponding to the constraints I.e., what are the causal layers for VQA task with a visual prompt?
2. How does the visual information flow to the causal layers (or token positions) corresponding to the constraints?
3. Can we edit MLLMs to incorporate rare knowledge into them? 


To answer the first question, we introduce MultimodalCausalTrace, which when combined with VQA-Constraints identifies the causal layers for the VQA task. Surprisingly, we find that very early MLP layers are causal for this task. However, when a similar query is given without visual prompt, the language model retrieves information from slightly later layers. This result shows that under the presence of a visual prompt, the language model retrieves information from its internal states differently. 


We then track the attention contributions from the visual tokens to the token positions corresponding to the causal layers. We find that only a subset of visual tokens (after the project layer) have a higher attention contributions. This potentially shows that a large amount of information is summarized by the projection layer. However, more investigation on this is needed to understand this transfer phenomenon.


Finally, we design a new and efficient model editing algorithm - MultEdit, which can edit the early MLP layers to incorporate rare knowledge into the MLLM. Although our objective is similar in essence to ROME, we note one main distinction:   Using ROME in the multimodal setup will require access to a Multimodal Wikipedia Matrix, however our method circumvents this. Empirical results with our editing algorithm shows that the model edits incorporated by MultEdit have high efficacy and does not harm the prior knowledge of the model majorly, when compared with other baselines.

üìö <small> **Paper**: ["Understanding Knowledge Storage and Transfer in Multimodal Language Models"](https://arxiv.org/abs/2406.04236) </small><br>

#### Future Directions

This work opens up a plethora of open questions. (i) Are there distinct circuits for different vision language tasks, and if so how do they differ? (ii) Can we perform batch-sequential model editing for MLLMs? (iii) What does the projection layer exactly do? We believe answering these questions will be critical towards understanding MLLMs in more depth and further desigining strategies to make them more reliable!
