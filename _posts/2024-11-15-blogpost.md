# Mechanistic Interpretability Beyond Language Models

## Abstract

In the recent times, a plethora of *mechanistic* interpretability methods have been developed, used towards understanding and modifying different aspects of language models. However, progress towards understanding *multimodal* and *vision* models have been slow when compared to the interest in mechanistic interpretability for language models.
In the past one year, Reliable AI Lab has been developing and adapting interpretability tools to understand the inner workings of multimodal and vision models, with a downstream emphasis on applications such as
+ removing copyrighted content in T2I models
+ improving compositionality
+ mitigating spurious correlations in vision models
+ updating rare knowledge in MLLMs

This blog provides a comprehensive overview of our year-long efforts in mechanistic interpretability beyond language models and discusses the open research problems in this area.


## <span style="font-variant: small-caps;">Causal Tracing</span> For Text-to-Image Generative Models


## <span style="font-variant: small-caps;">Localizing Knowledge</span>  in Cross-Attention Layers

Given the limitations of causal tracing for localizing knowledge within text encoders of recent models, we sought to develop a more generalizable approach for knowledge localization in text-to-image modelsâ€”one that could be effectively scaled and applied to modern architectures. In this work, we investiage whether knowledge representing *artistic styles*, *objects*, or *facts* can be localized in cross-attention layers of text-to-image models. Interestingly, we observed that among significant number of cross attention layers, only a select few play a key role in controlling the generation of specific concepts. In fact, the model heavily relies on information provided by those layers to generate those concepts. *e.g.*, we observed that in Stable Diffusion XL, layers 45-48 among 64 layers are responsible for Van Gogh style. Modifying the input only in these specific layers leads the UNet to produce an image that noticeably loses its Van Gogh characteristics. Localization within few layers enables an efficient and surgical model editiing method that aims to minimally modify cross-attention layers in those specified layers. This happens by carefully editing key value matrices within those layers, mapping original text prompts to representations that exclude the targeted concept. Importantly, the edited models retain their general utility and continue to generate high-quality images when prompted with general inputs. This approach worked for SD-v1.4, SD-v2, SD-XL, DeepFloyd and OpenJourney!

Please see more details about this work [here](https://t2i-knowledge-localization.github.io).

#### Future Directions

Say something here!

## Mechanistically Understand Model Components Across Different ViTs

In text-to-image models, we were able to analyze and control the generation of concepts via causal tracing by perturbing carefully chosen text tokens in the input which correspond to the concept of interest. However, in vision models with image inputs, it is more challenging to carefully control and perturb the chosen concepts, which means that causal tracing is less effective in this setting. Thus, rather than understanding the model from the input side, we aim to understand the model from the representation side. Specifically, we use the following approach:

1. We decompose the final representation as a sum of contributions from different model components. Furthermore, each of these component contributions can be decomposed over the image patch tokens.
2. We then map each of these contributions to CLIP space where they can be interpreted using CLIP's shared image-text representation space
3. Using the CLIP text encoder, we identify which components are responsible for encoding which concepts, and use them to retrieve or segment images, or ablate them to mitigate spurious correlations

Please see more details about this work [here](https://sriram.live/vit-decompose/).

#### Future Directions

Although we were able to localize certain concepts via this approach, there are still many open questions. For one, we found that the direct controbutions of model components are not limited to one role - they are responsible for jointly encoding many concepts along with other concepts. This makes it difficult to ablate them without affecting other concepts. One way to address this is to probably select a subspace within each component that is responsible for encoding a single concept. Another direction is to understand how these components interact with each other to encode concepts, as it is likely that concept components are built out of simpler ones.


## Compositionality in Diffusion Models

## Understanding Internal Components in Multimodal Language Models

